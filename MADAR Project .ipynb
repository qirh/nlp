{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Naif/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# LIBRARIES TO BE USED: \n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import operator\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.sparse import csr_matrix,hstack\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings('ignore')\n",
    "#Plot styling\n",
    "import seaborn as sns; sns.set()  # for plot styling\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (16, 9)\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/Naif/anaconda3/lib/python3.6/site-packages (3.3)\n",
      "Requirement already satisfied: six in /Users/Naif/anaconda3/lib/python3.6/site-packages (from nltk) (1.11.0)\n",
      "\u001b[33mYou are using pip version 18.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk  # Install natrual language toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk; nltk.download()  \n",
    "# If GUI does not show up: d, Enter, stopwords, Enter, q\n",
    "# If GUI does show up: Corpa > double click Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading training dataset for task-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41600, 2)\n",
      "(54000, 2)\n",
      "(101600, 2)\n",
      "number of different categories: 26\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>هناك ، أمام بيانات السائح تماما .</td>\n",
       "      <td>MSA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>لم اسمع بهذا العنوان من قبل بالقرب من هنا .</td>\n",
       "      <td>MSA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>استمر في السير في هذا الطريق حتى تجد صيدلية .</td>\n",
       "      <td>MSA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>كم تكلفة الإفطار ؟</td>\n",
       "      <td>MSA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>كيف أستطيع مساعدتك ؟</td>\n",
       "      <td>MSA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            text category\n",
       "0              هناك ، أمام بيانات السائح تماما .      MSA\n",
       "1    لم اسمع بهذا العنوان من قبل بالقرب من هنا .      MSA\n",
       "2  استمر في السير في هذا الطريق حتى تجد صيدلية .      MSA\n",
       "3                             كم تكلفة الإفطار ؟      MSA\n",
       "4                           كيف أستطيع مساعدتك ؟      MSA"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TRAIN DATA \n",
    "data1=pd.read_csv('MADAR-Corpus-26-train.tsv', sep='\\t', header=-1)\n",
    "data2=pd.read_csv('MADAR-Corpus-6-train.tsv', sep='\\t', header=-1)\n",
    "datadev2=pd.read_csv('MADAR-Corpus-6-dev.tsv', sep='\\t', header=-1)\n",
    "data1=data1.rename(columns={0: 'text', 1: 'category'})\n",
    "data2=data2.rename(columns={0: 'text', 1: 'category'})\n",
    "datadev2=datadev2.rename(columns={0: 'text', 1: 'category'})\n",
    "data_task1=pd.concat([data1,data2,datadev2])\n",
    "print(data1.shape)\n",
    "print(data2.shape)\n",
    "print(data_task1.shape)\n",
    "print(\"number of different categories: \" + str(len(set(data_task1.category.tolist()))))\n",
    "data_task1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading test dataset for task-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5200, 2)\n",
      "number of different categories: 26\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>بالمناسبة ، اسمي هيروش إيجيما .</td>\n",
       "      <td>MSA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>هذا القطار يتوقف في لاك فورست , أليس كذلك ؟</td>\n",
       "      <td>MSA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>هذا الكارت , حسناً ؟</td>\n",
       "      <td>MSA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>لم يخرج من الماكينة شيء .</td>\n",
       "      <td>MSA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>عندك أية شيء يمكن أن أتعاطه للطفح الجلدي ؟</td>\n",
       "      <td>MSA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text category\n",
       "0              بالمناسبة ، اسمي هيروش إيجيما .      MSA\n",
       "1  هذا القطار يتوقف في لاك فورست , أليس كذلك ؟      MSA\n",
       "2                         هذا الكارت , حسناً ؟      MSA\n",
       "3                    لم يخرج من الماكينة شيء .      MSA\n",
       "4   عندك أية شيء يمكن أن أتعاطه للطفح الجلدي ؟      MSA"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST DATA \n",
    "\n",
    "datadev1=pd.read_csv('MADAR-Corpus-26-dev.tsv', sep='\\t', header=-1)\n",
    "# datadev2=pd.read_csv('MADAR-Corpus-6-dev.tsv', sep='\\t', header=-1)\n",
    "datadev1=datadev1.rename(columns={0: 'text', 1: 'category'})\n",
    "# datadev2=datadev2.rename(columns={0: 'text', 1: 'category'})\n",
    "data_dev1=datadev1\n",
    "# print(datadev1.shape)\n",
    "# print(datadev2.shape)\n",
    "print(data_dev1.shape)\n",
    "print(\"number of different categories: \" + str(len(set(data_dev1.category.tolist()))))\n",
    "data_dev1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29869, 31)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>tweet_ID</th>\n",
       "      <th>language</th>\n",
       "      <th>user_country</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>ALE</th>\n",
       "      <th>ALG</th>\n",
       "      <th>ALX</th>\n",
       "      <th>AMM</th>\n",
       "      <th>ASW</th>\n",
       "      <th>...</th>\n",
       "      <th>MOS</th>\n",
       "      <th>MSA</th>\n",
       "      <th>MUS</th>\n",
       "      <th>RAB</th>\n",
       "      <th>RIY</th>\n",
       "      <th>SAL</th>\n",
       "      <th>SAN</th>\n",
       "      <th>SFX</th>\n",
       "      <th>TRI</th>\n",
       "      <th>TUN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qtri111</td>\n",
       "      <td>1094562990404354048</td>\n",
       "      <td>ar</td>\n",
       "      <td>Qatar</td>\n",
       "      <td>RT @marzoqi_w: @AhmadQatar تواصلت مع إدارة الص...</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1475</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0225</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0306</td>\n",
       "      <td>0.0476</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qtri111</td>\n",
       "      <td>1094559081078157313</td>\n",
       "      <td>ar</td>\n",
       "      <td>Qatar</td>\n",
       "      <td>RT @marzoqi_w: القول بان ينبغي عدم اثارة القضا...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9715</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0265</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>qtri111</td>\n",
       "      <td>1094552522855976960</td>\n",
       "      <td>ar</td>\n",
       "      <td>Qatar</td>\n",
       "      <td>RT @MUTARED: عندما يسجن العلماء والمفكرون والد...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0155</td>\n",
       "      <td>0.9843</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qtri111</td>\n",
       "      <td>1094552403062542336</td>\n",
       "      <td>ar</td>\n",
       "      <td>Qatar</td>\n",
       "      <td>RT @hfc_am1: ماقدرت أكمل الفيديو من اللي فيه !...</td>\n",
       "      <td>0.0012</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0338</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0179</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qtri111</td>\n",
       "      <td>1094551366599667712</td>\n",
       "      <td>ar</td>\n",
       "      <td>Qatar</td>\n",
       "      <td>RT @HDMshk: هاتان الدقيقتان ينبغي أن تدرس في ا...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      user             tweet_ID language user_country  \\\n",
       "0  qtri111  1094562990404354048       ar        Qatar   \n",
       "1  qtri111  1094559081078157313       ar        Qatar   \n",
       "2  qtri111  1094552522855976960       ar        Qatar   \n",
       "3  qtri111  1094552403062542336       ar        Qatar   \n",
       "4  qtri111  1094551366599667712       ar        Qatar   \n",
       "\n",
       "                                          tweet_text     ALE     ALG     ALX  \\\n",
       "0  RT @marzoqi_w: @AhmadQatar تواصلت مع إدارة الص...  0.0011  0.0001  0.0001   \n",
       "1  RT @marzoqi_w: القول بان ينبغي عدم اثارة القضا...  0.0000  0.0000  0.0000   \n",
       "2  RT @MUTARED: عندما يسجن العلماء والمفكرون والد...  0.0000  0.0000  0.0000   \n",
       "3  RT @hfc_am1: ماقدرت أكمل الفيديو من اللي فيه !...  0.0012  0.0000  0.0001   \n",
       "4  RT @HDMshk: هاتان الدقيقتان ينبغي أن تدرس في ا...  0.0000  0.0000  0.0000   \n",
       "\n",
       "      AMM     ASW   ...    MOS     MSA     MUS  RAB     RIY     SAL     SAN  \\\n",
       "0  0.1475  0.0001   ...    0.0  0.0005  0.0225  0.0  0.0306  0.0476  0.0002   \n",
       "1  0.0000  0.0000   ...    0.0  0.9715  0.0013  0.0  0.0006  0.0000  0.0265   \n",
       "2  0.0000  0.0000   ...    0.0  0.0155  0.9843  0.0  0.0001  0.0000  0.0000   \n",
       "3  0.0338  0.0010   ...    0.0  0.0000  0.8100  0.0  0.0895  0.0179  0.0002   \n",
       "4  0.0000  0.0000   ...    0.0  0.9999  0.0001  0.0  0.0000  0.0000  0.0000   \n",
       "\n",
       "      SFX     TRI     TUN  \n",
       "0  0.0003  0.0005  0.0007  \n",
       "1  0.0000  0.0000  0.0000  \n",
       "2  0.0000  0.0000  0.0000  \n",
       "3  0.0000  0.0000  0.0000  \n",
       "4  0.0000  0.0000  0.0000  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DEV # \n",
    "# data_twitter=pd.read_csv('new_dataset_DEV_tweets_features.csv', sep='\\t', header=0)\n",
    "# aa=data_twitter.iloc[:,3].str.split(',', expand=True)\n",
    "# aa.columns = ['ALE','ALG','ALX','AMM','ASW','BAG','BAS','BEI','BEN','CAI','DAM','DOH','FES','JED','JER','KHA','MOS','MSA','MUS','RAB','RIY','SAL','SAN','SFX','TRI','TUN']\n",
    "# data_twitter=data_twitter.drop(data_twitter.columns[3], axis=1)\n",
    "# data_twitter=data_twitter.join(aa)\n",
    "# data_twitter.columns = [\"user\", \"tweet_ID\", \"language\", \"user_country\", \"tweet_text\", 'ALE','ALG','ALX','AMM','ASW','BAG','BAS','BEI','BEN','CAI','DAM','DOH','FES','JED','JER','KHA','MOS','MSA','MUS','RAB','RIY','SAL','SAN','SFX','TRI','TUN']\n",
    "# data_twitter.to_csv('final_new_dataset_DEV_tweets_features.csv')\n",
    "# data_twitter\n",
    "twitter_data=pd.read_csv('final_new_dataset_DEV_tweets_features.csv')\n",
    "twitter_data=twitter_data.drop(twitter_data.columns[0], axis=1)\n",
    "print(twitter_data.shape)\n",
    "twitter_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(217592, 32)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>tweet_ID</th>\n",
       "      <th>language</th>\n",
       "      <th>user_country</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>ALE</th>\n",
       "      <th>ALG</th>\n",
       "      <th>ALX</th>\n",
       "      <th>AMM</th>\n",
       "      <th>ASW</th>\n",
       "      <th>...</th>\n",
       "      <th>MSA</th>\n",
       "      <th>MUS</th>\n",
       "      <th>RAB</th>\n",
       "      <th>RIY</th>\n",
       "      <th>SAL</th>\n",
       "      <th>SAN</th>\n",
       "      <th>SFX</th>\n",
       "      <th>TRI</th>\n",
       "      <th>TUN</th>\n",
       "      <th>is_retweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uJ4DPgC8LZxsFJ6</td>\n",
       "      <td>1094598211673690112</td>\n",
       "      <td>ar</td>\n",
       "      <td>Oman</td>\n",
       "      <td>RT @waha1950: (أَمْسَيْنَا وَأَمْسَى الْمُلْكُ...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.9976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uJ4DPgC8LZxsFJ6</td>\n",
       "      <td>1094598132720193543</td>\n",
       "      <td>ar</td>\n",
       "      <td>Oman</td>\n",
       "      <td>RT @Mr_smmsm: اللهم إنى أعوذ بك من الهم و الحز...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9648</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0088</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>uJ4DPgC8LZxsFJ6</td>\n",
       "      <td>1094597914985459713</td>\n",
       "      <td>ar</td>\n",
       "      <td>Oman</td>\n",
       "      <td>RT @waha1950: (مَن قال حين يُمسِي ثلاث مرات أَ...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.4342</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0305</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uJ4DPgC8LZxsFJ6</td>\n",
       "      <td>1094597887185637376</td>\n",
       "      <td>ar</td>\n",
       "      <td>Oman</td>\n",
       "      <td>RT @90_cutte: لا إله إلا أنت سبحانك إني كنت من...</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0079</td>\n",
       "      <td>0.0106</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0235</td>\n",
       "      <td>0.0099</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>uJ4DPgC8LZxsFJ6</td>\n",
       "      <td>1094597862879580162</td>\n",
       "      <td>ar</td>\n",
       "      <td>Oman</td>\n",
       "      <td>RT @NF7AT11: قال ﷺ: من صلى عليَّ صلاة واحدة صل...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9930</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              user             tweet_ID language user_country  \\\n",
       "0  uJ4DPgC8LZxsFJ6  1094598211673690112       ar         Oman   \n",
       "1  uJ4DPgC8LZxsFJ6  1094598132720193543       ar         Oman   \n",
       "2  uJ4DPgC8LZxsFJ6  1094597914985459713       ar         Oman   \n",
       "3  uJ4DPgC8LZxsFJ6  1094597887185637376       ar         Oman   \n",
       "4  uJ4DPgC8LZxsFJ6  1094597862879580162       ar         Oman   \n",
       "\n",
       "                                          tweet_text     ALE     ALG  ALX  \\\n",
       "0  RT @waha1950: (أَمْسَيْنَا وَأَمْسَى الْمُلْكُ...  0.0000  0.0000  0.0   \n",
       "1  RT @Mr_smmsm: اللهم إنى أعوذ بك من الهم و الحز...  0.0000  0.9648  0.0   \n",
       "2  RT @waha1950: (مَن قال حين يُمسِي ثلاث مرات أَ...  0.0000  0.0000  0.0   \n",
       "3  RT @90_cutte: لا إله إلا أنت سبحانك إني كنت من...  0.0015  0.0000  0.0   \n",
       "4  RT @NF7AT11: قال ﷺ: من صلى عليَّ صلاة واحدة صل...  0.0000  0.0000  0.0   \n",
       "\n",
       "      AMM     ASW     ...         MSA     MUS  RAB     RIY     SAL     SAN  \\\n",
       "0  0.0000  0.0000     ...      0.0002  0.9976  0.0  0.0010  0.0001  0.0001   \n",
       "1  0.0001  0.0000     ...      0.0009  0.0034  0.0  0.0075  0.0025  0.0002   \n",
       "2  0.0001  0.0000     ...      0.0205  0.4342  0.0  0.0305  0.0013  0.0016   \n",
       "3  0.0079  0.0106     ...      0.0235  0.0099  0.0  0.0001  0.0002  0.0000   \n",
       "4  0.0000  0.0000     ...      0.9930  0.0063  0.0  0.0004  0.0000  0.0000   \n",
       "\n",
       "      SFX     TRI     TUN  is_retweet  \n",
       "0  0.0000  0.0000  0.0000           1  \n",
       "1  0.0046  0.0015  0.0088           1  \n",
       "2  0.0000  0.0001  0.0000           1  \n",
       "3  0.0000  0.0000  0.0000           1  \n",
       "4  0.0000  0.0000  0.0000           1  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_twitter2=pd.read_csv('new_dataset_TRAIN_tweets_features.csv', sep='\\t', header=0)\n",
    "# aa=data_twitter2.iloc[:,3].str.split(',', expand=True)\n",
    "# aa.columns = ['ALE','ALG','ALX','AMM','ASW','BAG','BAS','BEI','BEN','CAI','DAM','DOH','FES','JED','JER','KHA','MOS','MSA','MUS','RAB','RIY','SAL','SAN','SFX','TRI','TUN']\n",
    "# data_twitter2=data_twitter2.drop(data_twitter2.columns[3], axis=1)\n",
    "# data_twitter2=data_twitter2.join(aa)\n",
    "# data_twitter2.columns = [\"user\", \"tweet_ID\", \"language\", \"user_country\", \"tweet_text\", 'ALE','ALG','ALX','AMM','ASW','BAG','BAS','BEI','BEN','CAI','DAM','DOH','FES','JED','JER','KHA','MOS','MSA','MUS','RAB','RIY','SAL','SAN','SFX','TRI','TUN']\n",
    "# data_twitter2.to_csv('final_new_dataset_TRAIN_tweets_features.csv')\n",
    "# data_twitter2\n",
    "\n",
    "twitter_data2=pd.read_csv('final_new_dataset_TRAIN_tweets_features_wRETWEETnum.csv')\n",
    "twitter_data2=twitter_data2.drop(twitter_data2.columns[0], axis=1)\n",
    "print(twitter_data2.shape)\n",
    "data=twitter_data2\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2180, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#1 User</th>\n",
       "      <th>#2 Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uJ4DPgC8LZxsFJ6</td>\n",
       "      <td>Oman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NasrMoza</td>\n",
       "      <td>Qatar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandaHabib</td>\n",
       "      <td>Jordan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>h1kFsEDzLswl4zM</td>\n",
       "      <td>Qatar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hu_dly</td>\n",
       "      <td>Iraq</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           #1 User #2 Label\n",
       "0  uJ4DPgC8LZxsFJ6     Oman\n",
       "1         NasrMoza    Qatar\n",
       "2       RandaHabib   Jordan\n",
       "3  h1kFsEDzLswl4zM    Qatar\n",
       "4           hu_dly     Iraq"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_label=pd.read_csv('MADAR-Twitter-Subtask-2.TRAIN.user-label.tsv', sep='\\t', header=0)\n",
    "print(user_label.shape)\n",
    "user_label.head() #seems useless since it is already in the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['هيا', 'لوما', 'لو', 'هي', 'نحو', 'كيف', 'إليكم', 'أنتن', 'أو', 'بك', 'بمن', 'تين', 'كلاهما', 'له', 'هنا', 'ومن', 'والذين', 'أولاء', 'بس', 'لي', 'هن', 'إليك', 'تي', 'إما', 'إنما', 'آي', 'لسن', 'منه', 'هيهات', 'هو', 'أف', 'لكيلا', 'إذ', 'بخ', 'ليت', 'مه', 'هذه', 'أنت', 'بل', 'هؤلاء', 'التي', 'ذلكما', 'كما', 'لعل', 'أوه', 'قد', 'ذينك', 'أولئك', 'لست', 'وإذا', 'لسنا', 'هذي', 'أما', 'آها', 'سوف', 'مهما', 'من', 'لهما', 'لن', 'بها', 'ذلك', 'ذا', 'لنا', 'منها', 'ها', 'ألا', 'إن', 'بيد', 'هذان', 'ذوا', 'اللاتي', 'عل', 'ليست', 'أم', 'الذين', 'مذ', 'فإن', 'خلا', 'لولا', 'عليه', 'ليس', 'به', 'منذ', 'بماذا', 'بعد', 'شتان', 'إلى', 'إذن', 'بلى', 'إليكن', 'بكما', 'ذي', 'على', 'لما', 'هيت', 'كم', 'بعض', 'ثم', 'هاتي', 'لستن', 'وما', 'أنا', 'فمن', 'بكن', 'كلتا', 'نحن', 'ولو', 'لكنما', 'لم', 'لستما', 'فيه', 'اللواتي', 'وإن', 'وهو', 'ليسوا', 'هاتين', 'عدا', 'كأي', 'بي', 'ذواتي', 'كيت', 'لستم', 'مع', 'حبذا', 'إليكما', 'إنه', 'نعم', 'تينك', 'حيثما', 'متى', 'آه', 'دون', 'ته', 'ذان', 'هما', 'لهم', 'إي', 'والذي', 'ذواتا', 'لكي', 'كأين', 'هذا', 'كي', 'اللائي', 'ذين', 'يا', 'كلما', 'اللتيا', 'تلكم', 'سوى', 'لا', 'ريث', 'كذا', 'حتى', 'لاسيما', 'حين', 'كليكما', 'كلا', 'اللذان', 'ذو', 'بنا', 'اللتان', 'ليسا', 'هاك', 'ذانك', 'إلا', 'عليك', 'فإذا', 'ذلكن', 'كأن', 'لئن', 'ولكن', 'فيما', 'عن', 'لكم', 'كذلك', 'كليهما', 'بهما', 'فلا', 'حيث', 'هنالك', 'تلك', 'عسى', 'في', 'أنتم', 'أكثر', 'تلكما', 'ولا', 'أقل', 'لدى', 'هل', 'غير', 'هاته', 'ثمة', 'بين', 'وإذ', 'هاهنا', 'بما', 'الذي', 'فيم', 'لهن', 'إذا', 'فيها', 'أنتما', 'هكذا', 'لك', 'بهم', 'لكن', 'إذما', 'هلا', 'هاتان', 'إنا', 'كيفما', 'اللتين', 'ماذا', 'كل', 'أينما', 'ذه', 'ذلكم', 'أنى', 'أين', 'هناك', 'ممن', 'بهن', 'أن', 'لها', 'أيها', 'حاشا', 'هذين', 'مما', 'لكما', 'ما', 'كأنما', 'اللذين', 'هم', 'بكم', 'أي', 'ليستا', 'ذاك', 'عما', 'إيه', 'ذات', 'عند']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words('Arabic'))\n",
    "noise=list(stops)\n",
    "print(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_duplicate(d):\n",
    "#     d = d.split(\" \")\n",
    "#     UniqW = Counter(d) \n",
    "#     s = \" \".join(UniqW.keys()) \n",
    "#     return s\n",
    "\n",
    "noise_list = ['.','!','،','؟'] #punctuation \n",
    "def remove_noise(input_text):\n",
    "    words = input_text.split() \n",
    "    noise_free_words = [word for word in words if word not in noise_list] \n",
    "    noise_free_text = \" \".join(noise_free_words) \n",
    "    return noise_free_text\n",
    "\n",
    "def remove_english(a): #remove any english word \n",
    "    c=[]\n",
    "    for string in a.split():\n",
    "        if not re.search(r'[a-zA-Z]', string):\n",
    "            c.append(string)\n",
    "    aa=' '.join(c)\n",
    "    return aa \n",
    "\n",
    "def char_gram(b,n=5): #charecter level ngrams \n",
    "    last=[]\n",
    "    for n in range(2,n+1,1):\n",
    "        x[n]=[b[i:i+n] for i in range(len(b)-n+1)]\n",
    "        last+=x[n]\n",
    "    last+=b.split()\n",
    "    return \"<some_space>\".join(last)\n",
    "\n",
    "#char_gram calculates all ngrams from n=2 to the specified n. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'أم<some_space>ما<some_space>ام<some_space>م <some_space> ب<some_space>بي<some_space>يا<some_space>ان<some_space>نا<some_space>ات<some_space>ت <some_space> ا<some_space>ال<some_space>لس<some_space>سا<some_space>ائ<some_space>ئح<some_space>ح <some_space> ت<some_space>تم<some_space>ما<some_space>ام<some_space>ما<some_space>أما<some_space>مام<some_space>ام <some_space>م ب<some_space> بي<some_space>بيا<some_space>يان<some_space>انا<some_space>نات<some_space>ات <some_space>ت ا<some_space> ال<some_space>الس<some_space>لسا<some_space>سائ<some_space>ائح<some_space>ئح <some_space>ح ت<some_space> تم<some_space>تما<some_space>مام<some_space>اما<some_space>أمام<some_space>مام <some_space>ام ب<some_space>م بي<some_space> بيا<some_space>بيان<some_space>يانا<some_space>انات<some_space>نات <some_space>ات ا<some_space>ت ال<some_space> الس<some_space>السا<some_space>لسائ<some_space>سائح<some_space>ائح <some_space>ئح ت<some_space>ح تم<some_space> تما<some_space>تمام<some_space>ماما<some_space>أمام <some_space>مام ب<some_space>ام بي<some_space>م بيا<some_space> بيان<some_space>بيانا<some_space>يانات<some_space>انات <some_space>نات ا<some_space>ات ال<some_space>ت الس<some_space> السا<some_space>السائ<some_space>لسائح<some_space>سائح <some_space>ائح ت<some_space>ئح تم<some_space>ح تما<some_space> تمام<some_space>تماما<some_space>أمام<some_space>بيانات<some_space>السائح<some_space>تماما'"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example of char_gram  \n",
    "b='أمام بيانات السائح تماما'\n",
    "char_gram(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>هن&lt;some_space&gt;نا&lt;some_space&gt;اك&lt;some_space&gt;ك &lt;s...</td>\n",
       "      <td>MSA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>لم&lt;some_space&gt;م &lt;some_space&gt; ا&lt;some_space&gt;اس&lt;s...</td>\n",
       "      <td>MSA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>اس&lt;some_space&gt;ست&lt;some_space&gt;تم&lt;some_space&gt;مر&lt;s...</td>\n",
       "      <td>MSA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>كم&lt;some_space&gt;م &lt;some_space&gt; ت&lt;some_space&gt;تك&lt;s...</td>\n",
       "      <td>MSA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>كي&lt;some_space&gt;يف&lt;some_space&gt;ف &lt;some_space&gt; أ&lt;s...</td>\n",
       "      <td>MSA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text category\n",
       "0  هن<some_space>نا<some_space>اك<some_space>ك <s...      MSA\n",
       "1  لم<some_space>م <some_space> ا<some_space>اس<s...      MSA\n",
       "2  اس<some_space>ست<some_space>تم<some_space>مر<s...      MSA\n",
       "3  كم<some_space>م <some_space> ت<some_space>تك<s...      MSA\n",
       "4  كي<some_space>يف<some_space>ف <some_space> أ<s...      MSA"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply the three preprocessing functions to the training dataset\n",
    "\n",
    "data_task1['text']=data_task1['text'].apply(remove_noise)\n",
    "data_task1['text']=data_task1['text'].apply(remove_english)\n",
    "data_task1['text']=data_task1['text'].apply(char_gram)\n",
    "\n",
    "data_task1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>با&lt;some_space&gt;ال&lt;some_space&gt;لم&lt;some_space&gt;من&lt;s...</td>\n",
       "      <td>MSA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>هذ&lt;some_space&gt;ذا&lt;some_space&gt;ا &lt;some_space&gt; ا&lt;s...</td>\n",
       "      <td>MSA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>هذ&lt;some_space&gt;ذا&lt;some_space&gt;ا &lt;some_space&gt; ا&lt;s...</td>\n",
       "      <td>MSA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>لم&lt;some_space&gt;م &lt;some_space&gt; ي&lt;some_space&gt;يخ&lt;s...</td>\n",
       "      <td>MSA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>عن&lt;some_space&gt;ند&lt;some_space&gt;دك&lt;some_space&gt;ك &lt;s...</td>\n",
       "      <td>MSA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text category\n",
       "0  با<some_space>ال<some_space>لم<some_space>من<s...      MSA\n",
       "1  هذ<some_space>ذا<some_space>ا <some_space> ا<s...      MSA\n",
       "2  هذ<some_space>ذا<some_space>ا <some_space> ا<s...      MSA\n",
       "3  لم<some_space>م <some_space> ي<some_space>يخ<s...      MSA\n",
       "4  عن<some_space>ند<some_space>دك<some_space>ك <s...      MSA"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply the three preprocessing functions to the test dataset\n",
    "\n",
    "data_dev1['text']=data_dev1['text'].apply(remove_noise)\n",
    "data_dev1['text']=data_dev1['text'].apply(remove_english)\n",
    "data_dev1['text']=data_dev1['text'].apply(char_gram)\n",
    "\n",
    "data_dev1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_task1[data_task1.category == 'CAI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS FOR TASK 2! - to add a feature of whether the tweet is a retweet. \n",
    "# Add column to say whether the tweet is a retweet or not - Takes forever\n",
    "# Use the other dataset \n",
    "data[\"is_retweet\"] = 0\n",
    "for i in range(data.shape[0]):\n",
    "    if \"RT\" in data.loc[i, \"tweet_text\"]:\n",
    "        data.loc[i, \"is_retweet\"] = 1\n",
    "    else:\n",
    "        data.loc[i, \"is_retweet\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet_ID</th>\n",
       "      <th>language</th>\n",
       "      <th>user_country</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>ALE</th>\n",
       "      <th>ALG</th>\n",
       "      <th>ALX</th>\n",
       "      <th>AMM</th>\n",
       "      <th>...</th>\n",
       "      <th>MSA</th>\n",
       "      <th>MUS</th>\n",
       "      <th>RAB</th>\n",
       "      <th>RIY</th>\n",
       "      <th>SAL</th>\n",
       "      <th>SAN</th>\n",
       "      <th>SFX</th>\n",
       "      <th>TRI</th>\n",
       "      <th>TUN</th>\n",
       "      <th>is_retweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>uJ4DPgC8LZxsFJ6</td>\n",
       "      <td>1094598211673690112</td>\n",
       "      <td>ar</td>\n",
       "      <td>Oman</td>\n",
       "      <td>(أَمْسَيْنَا وَأَمْسَى الْمُلْكُ لِله وَالْحَم...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.9976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>uJ4DPgC8LZxsFJ6</td>\n",
       "      <td>1094598132720193543</td>\n",
       "      <td>ar</td>\n",
       "      <td>Oman</td>\n",
       "      <td>اللهم إنى أعوذ الهم و الحزن و أعوذ العجز و الك...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9648</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0088</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>uJ4DPgC8LZxsFJ6</td>\n",
       "      <td>1094597914985459713</td>\n",
       "      <td>ar</td>\n",
       "      <td>Oman</td>\n",
       "      <td>(مَن قال يُمسِي ثلاث مرات أَعوذ بِكَلماتِ اللَ...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.4342</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0305</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>uJ4DPgC8LZxsFJ6</td>\n",
       "      <td>1094597887185637376</td>\n",
       "      <td>ar</td>\n",
       "      <td>Oman</td>\n",
       "      <td>إله سبحانك إني كنت الظالمين</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0079</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0235</td>\n",
       "      <td>0.0099</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>uJ4DPgC8LZxsFJ6</td>\n",
       "      <td>1094597862879580162</td>\n",
       "      <td>ar</td>\n",
       "      <td>Oman</td>\n",
       "      <td>قال ﷺ: صلى عليَّ صلاة واحدة صلى الله عشرا ﷺﷺﷺﷺ...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9930</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0             user             tweet_ID language user_country  \\\n",
       "0           0  uJ4DPgC8LZxsFJ6  1094598211673690112       ar         Oman   \n",
       "1           1  uJ4DPgC8LZxsFJ6  1094598132720193543       ar         Oman   \n",
       "2           2  uJ4DPgC8LZxsFJ6  1094597914985459713       ar         Oman   \n",
       "3           3  uJ4DPgC8LZxsFJ6  1094597887185637376       ar         Oman   \n",
       "4           4  uJ4DPgC8LZxsFJ6  1094597862879580162       ar         Oman   \n",
       "\n",
       "                                          tweet_text     ALE     ALG  ALX  \\\n",
       "0  (أَمْسَيْنَا وَأَمْسَى الْمُلْكُ لِله وَالْحَم...  0.0000  0.0000  0.0   \n",
       "1  اللهم إنى أعوذ الهم و الحزن و أعوذ العجز و الك...  0.0000  0.9648  0.0   \n",
       "2  (مَن قال يُمسِي ثلاث مرات أَعوذ بِكَلماتِ اللَ...  0.0000  0.0000  0.0   \n",
       "3                        إله سبحانك إني كنت الظالمين  0.0015  0.0000  0.0   \n",
       "4  قال ﷺ: صلى عليَّ صلاة واحدة صلى الله عشرا ﷺﷺﷺﷺ...  0.0000  0.0000  0.0   \n",
       "\n",
       "      AMM     ...         MSA     MUS  RAB     RIY     SAL     SAN     SFX  \\\n",
       "0  0.0000     ...      0.0002  0.9976  0.0  0.0010  0.0001  0.0001  0.0000   \n",
       "1  0.0001     ...      0.0009  0.0034  0.0  0.0075  0.0025  0.0002  0.0046   \n",
       "2  0.0001     ...      0.0205  0.4342  0.0  0.0305  0.0013  0.0016  0.0000   \n",
       "3  0.0079     ...      0.0235  0.0099  0.0  0.0001  0.0002  0.0000  0.0000   \n",
       "4  0.0000     ...      0.9930  0.0063  0.0  0.0004  0.0000  0.0000  0.0000   \n",
       "\n",
       "      TRI     TUN  is_retweet  \n",
       "0  0.0000  0.0000           1  \n",
       "1  0.0015  0.0088           1  \n",
       "2  0.0001  0.0000           1  \n",
       "3  0.0000  0.0000           1  \n",
       "4  0.0000  0.0000           1  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FINAL DATASET FOR TASK 2 WITH RETWEET COLUMN \n",
    "# data.to_csv('final2_new_dataset_TRAIN_tweets_features_wRETWEET.csv')\n",
    "data=pd.read_csv('final2_new_dataset_TRAIN_tweets_features_wRETWEET.csv')\n",
    "twitter_data2=twitter_data2.drop(twitter_data2.columns[0], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing of different models **STARTS HERE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>هن&lt;some_space&gt;نا&lt;some_space&gt;اك&lt;some_space&gt;ك &lt;s...</td>\n",
       "      <td>MSA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>لم&lt;some_space&gt;م &lt;some_space&gt; ا&lt;some_space&gt;اس&lt;s...</td>\n",
       "      <td>MSA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>اس&lt;some_space&gt;ست&lt;some_space&gt;تم&lt;some_space&gt;مر&lt;s...</td>\n",
       "      <td>MSA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>كم&lt;some_space&gt;م &lt;some_space&gt; ت&lt;some_space&gt;تك&lt;s...</td>\n",
       "      <td>MSA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>كي&lt;some_space&gt;يف&lt;some_space&gt;ف &lt;some_space&gt; أ&lt;s...</td>\n",
       "      <td>MSA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text category\n",
       "0  هن<some_space>نا<some_space>اك<some_space>ك <s...      MSA\n",
       "1  لم<some_space>م <some_space> ا<some_space>اس<s...      MSA\n",
       "2  اس<some_space>ست<some_space>تم<some_space>مر<s...      MSA\n",
       "3  كم<some_space>م <some_space> ت<some_space>تك<s...      MSA\n",
       "4  كي<some_space>يف<some_space>ف <some_space> أ<s...      MSA"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_task1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Vectorization of text for task 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((101600, 419092), (5200, 419092))"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_train, X_test, Y_train, Y_test = train_test_split(data_task1['text'], data_task1['category'], test_size=0.2,random_state=10)\n",
    "\n",
    "count = CountVectorizer(tokenizer = lambda x: x.split(\"<some_space>\"), analyzer=\"word\")\n",
    "# count = CountVectorizer(min_df=3) this if you'd like to the words that have repeated only 3 times. \n",
    "# it has been tried and resulted with a less score for SVM \n",
    "X_train = count.fit_transform(data_task1['text'])\n",
    "X_test = count.transform(data_dev1['text'])\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((101600, 146616), (5200, 146616))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<1x146616 sparse matrix of type '<class 'numpy.int64'>'\n",
       " \twith 79 stored elements in Compressed Sparse Row format>,\n",
       " 0    هن<some_space>نا<some_space>اك<some_space>ك <s...\n",
       " 0    شن<some_space>نا<some_space>اه<some_space>هو<s...\n",
       " 0    بغ<some_space>غي<some_space>يت<some_space>ت <s...\n",
       " Name: text, dtype: object)"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0],data_task1['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = data_task1['category']\n",
    "Y_test = data_dev1['category']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummization of the labels\n",
    "dummies=pd.get_dummies(Y_train,prefix=\"\",prefix_sep=\"\",sparse=True)\n",
    "labels=dummies.columns\n",
    "Z=dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUPPORT VECTOR MACHINE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Cs = [0.01,0.1,1,10]\n",
    "# val_error = []\n",
    "\n",
    "# for C in Cs:\n",
    "\n",
    "cross_val = cross_val_score(model, X_train, Y_train, cv=5)\n",
    "val_err=cross_val.mean()\n",
    "val_error.append(val_err)\n",
    "print(C,val_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Pick C=1 as best value for hyperparameter and retrain on whole training set\n",
    "model=SVC(C=1, kernel='linear')\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5486538461538462"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred = model.predict(X_test)\n",
    "np.mean(Y_pred==Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(Y_pred, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5521264290839432"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scoreSVM=f1_score(Y_test, Y_pred, average='weighted')  \n",
    "scoreSVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5830062191509544"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "y_pred2 = logreg.predict(X_test)\n",
    "scorelogreg=f1_score(Y_test, y_pred2, average='weighted')  \n",
    "scorelogreg\n",
    "# for char_gram with n = 3, the f-1 score is 0.6642045984948366, second trial = 0.6714308807323026\n",
    "# for char_gram with n = 5, the f-1 score is 0.5407657629111126\n",
    "# ------------ \n",
    "## ACTUAL SCORES ON DEV CORPUS 26: \n",
    "# WITH n=4 for char-level grams (including n=3,2) and word unigrams: F-1 score = 0.581698\n",
    "# WITH n=3 only and word unigrams: F-1 score = 0.56254\n",
    "# WITH n=3 including n=2 with word unigrams: F-1 score = 0.56875\n",
    "# WITH n=4 and word unigrams: F-1 score = 0.56525\n",
    "# WITH n=4 ONLY: F-1 score = 0.5715\n",
    "# WITH n=5 for char-level grams (including n=4,3,2) and word unigrams: f1 score = 0.5830\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='multi:softprob', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "modelXGB = XGBClassifier()\n",
    "modelXGB.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predXGB = model.predict(X_test)\n",
    "scoreXGB=f1_score(Y_test, y_predXGB, average='weighted')  \n",
    "scorelogreg=f1_score(Y_test, y_pred2, average='weighted')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM F1-score: 0.5521264290839432\n",
      "Log Reg F1-score: 0.5625453758463514\n"
     ]
    }
   ],
   "source": [
    "print('SVM F1-score: ' + str(scoreSVM))\n",
    "print('Log Reg F1-score: ' + str(scorelogreg))\n",
    "print('XGBoost F1-score: ' + str(scoreXGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
